diff --git a/base_experiment/config/test.yaml b/base_experiment/config/test.yaml
index 9a7c809..720107a 100644
--- a/base_experiment/config/test.yaml
+++ b/base_experiment/config/test.yaml
@@ -1,9 +1,9 @@
 "LR": 5e-4
-"NUM_ENVS": 3
-"NUM_STEPS": 360
+"NUM_ENVS": 2
+"NUM_STEPS": 100
 "TOTAL_TIMESTEPS": 1e5
 "UPDATE_EPOCHS": 4
-"MINIBATCH_SIZE": 30
+"MINIBATCH_SIZE": 25
 # "NUM_MINIBATCHES": 3  # This must be a whole divisor of NUM_STEPS # Deprecated, doing this programmatically as NUM_STEPS/MINIBATCH_SIZE
 "GAMMA": 0.99
 "GAE_LAMBDA": 0.95
@@ -17,6 +17,6 @@
 "ANNEAL_LR": False
 
 # WandB Params
-"WANDB_MODE": "disabled"
+"WANDB_MODE": "online"
 "ENTITY": ""
 "PROJECT": "signification-game"
diff --git a/base_experiment/ippo_ff.py b/base_experiment/ippo_ff.py
index 12c0ff7..94cdad6 100644
--- a/base_experiment/ippo_ff.py
+++ b/base_experiment/ippo_ff.py
@@ -354,7 +354,7 @@ def make_train(config):
     
     # For the learning rate
     def linear_schedule(count):
-        frac = 1.0 - (count // (config["NUM_MINIBATCHES"] * config["UPDATE_EPOCHS"])) / config["NUM_UPDATES"]
+        frac = 1.0 - (count // (config["NUM_MINIBATCHES"] * config["UPDATE_EPOCHS"])) / config["NUM_UPDATES"]   # I don't know exactly how this works.
         return config["LR"] * frac
 
     def train(rng):
@@ -373,6 +373,7 @@ def make_train(config):
         
         # TRAIN LOOP
         def _update_step(runner_state, update_step, env, config):
+            # runner_state is actually a tuple of runner_states, one per agent
             
             # COLLECT TRAJECTORIES
             runner_state, transition_batch = jax.lax.scan(lambda rs, _: env_step(rs, env, config), runner_state, None, config['NUM_STEPS'] + 1)
@@ -458,7 +459,7 @@ def make_train(config):
                 #             * config["NUM_STEPS"],
                 #         }
                 #     )
-                # metric["update_steps"] = update_steps
+                # # metric["update_steps"] = update_steps
                 # jax.experimental.io_callback(callback, None, metric)
 
                 return new_listener_train_state_i, total_loss
@@ -489,7 +490,20 @@ def make_train(config):
             
             listener_map_outputs = tuple(map(lambda i: _update_a_listener(i, listener_train_state, listener_trans_batch, listener_advantages, listener_targets), range(len(listener_rngs))))
             listener_train_state = tuple([lmo[0] for lmo in listener_map_outputs])
-            # Should probabyl be logging lmo[1] values, which is total loss per listener
+            listener_loss = tuple([lmo[1] for lmo in listener_map_outputs])
+
+            def callback(metrics):
+                # agent, total_loss, (value_loss, loss_actor, entropy)
+                ll, r = metrics
+
+                loss_dict = {f"total loss for listener {i}": jnp.mean(ll[i][0]).item() for i in range(len(ll))}
+                loss_dict.update({f"value loss for listener {i}": jnp.mean(ll[i][1][0]).item() for i in range(len(ll))})
+                loss_dict.update({f"actor loss for listener {i}": jnp.mean(ll[i][1][1]).item() for i in range(len(ll))})
+                loss_dict.update({f"entropy for listener {i}": jnp.mean(ll[i][1][2]).item() for i in range(len(ll))})
+                # loss_dict["average_loss"] = jnp.mean(ll)
+                wandb.log(loss_dict)
+
+            jax.experimental.io_callback(callback, None, (listener_loss, listener_trans_batch.reward))
 
             runner_state = (listener_train_state, log_env_state, last_obs, last_done, rng)
             return runner_state, update_step + 1
@@ -509,6 +523,14 @@ def make_train(config):
 
 @hydra.main(version_base=None, config_path="config", config_name="test")
 def test(config):
+    wandb.init(
+        # entity=config["ENTITY"],
+        project=config["PROJECT"],
+        tags=["test"],
+        config=config,
+        mode=config["WANDB_MODE"],
+        save_code=True
+    )
     with jax.profiler.trace("/tmp/jax-trace", create_perfetto_link=True):
         config = OmegaConf.to_container(config) 
         rng = jax.random.PRNGKey(50)
@@ -518,26 +540,26 @@ def test(config):
 
 @hydra.main(version_base=None, config_path="config", config_name="test")
 def main(config):
-    config = OmegaConf.to_container(config) 
-
-    # Setting aside wandb for now.
-    # wandb.init(
-    #     entity=config["ENTITY"],
-    #     project=config["PROJECT"],
-    #     tags=["IPPO", "FF", config["ENV_NAME"]],
-    #     config=config,
-    #     mode=config["WANDB_MODE"],
-    # )
-    
-    rng = jax.random.PRNGKey(50)
-    # train_jit = jax.jit(make_train(config), device=jax.devices()[0]) # The environment may or may not be jittable.
-    train = make_train(config)
-    out = train(rng)
-    print("Done")
+    # print(config)
+    wandb.init(
+        # entity=config["ENTITY"],
+        project=config["PROJECT"],
+        tags=["main"],
+        # config=config,
+        mode=config["WANDB_MODE"],
+        save_code=True
+    )
+    with jax.profiler.trace("/tmp/jax-trace", create_perfetto_link=True):
+        config = OmegaConf.to_container(config) 
+        rng = jax.random.PRNGKey(50)
+        # train_jit = jax.jit(make_train(config), device=jax.devices()[0]) # The environment may or may not be jittable.
+        train = make_train(config)
+        out = train(rng)
+        print("Done")
 
 
 if __name__ == "__main__":
-    test()
+    main()
     '''results = out["metrics"]["returned_episode_returns"].mean(-1).reshape(-1)
     jnp.save('hanabi_results', results)
     plt.plot(results)
